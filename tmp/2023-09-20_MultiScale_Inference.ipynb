{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../data'\n",
    "path_ckpt = '../ckpt/1695772427_pl/last_ckpt.bin'\n",
    "outside1_fname = '/home/eunwoo/experiment/PSSC/Oneformer(instance)/result/background6/background_fish.pickle'\n",
    "outside2_fname = '/home/eunwoo/experiment/PSSC/Oneformer(instance)/result/background6/background_total.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from segformers.utils import custom_cmap, rle_encode\n",
    "from segformers.detectors import Backgroud_detector\n",
    "from segformers.networks import SegFormer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dict = torch.load(path_ckpt)\n",
    "model = SegFormer\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def slide_inference(images, model, num_classes=13, crop_size=(1024, 1024), stride=(768, 768)):\n",
    "    h_stride, w_stride = stride\n",
    "    h_crop, w_crop = crop_size\n",
    "    batch_size, _, h_img, w_img = images.size()\n",
    "\n",
    "    h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1\n",
    "    w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1\n",
    "    preds = images.new_zeros((batch_size, num_classes, h_img, w_img))\n",
    "    count_mat = images.new_zeros((batch_size, 1, h_img, w_img))\n",
    "    for h_idx in range(h_grids):\n",
    "        for w_idx in range(w_grids):\n",
    "            y1 = h_idx * h_stride\n",
    "            x1 = w_idx * w_stride\n",
    "            y2 = min(y1 + h_crop, h_img)\n",
    "            x2 = min(x1 + w_crop, w_img)\n",
    "            y1 = max(y2 - h_crop, 0)\n",
    "            x1 = max(x2 - w_crop, 0)\n",
    "            crop_img = images[:, :, y1:y2, x1:x2]\n",
    "\n",
    "            crop_seg_logit = model(pixel_values=crop_img)[0]\n",
    "            crop_seg_logit = F.interpolate(\n",
    "                crop_seg_logit,\n",
    "                size=crop_size,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )\n",
    "            preds += F.pad(crop_seg_logit,\n",
    "                            (int(x1), int(preds.shape[3] - x2), int(y1),\n",
    "                            int(preds.shape[2] - y2)))\n",
    "\n",
    "            count_mat[:, :, y1:y2, x1:x2] += 1\n",
    "    assert (count_mat == 0).sum() == 0\n",
    "    seg_logits = preds / count_mat\n",
    "\n",
    "    return preds, count_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outside1_fname, 'rb') as f:\n",
    "    outside1 = pickle.load(f)\n",
    "\n",
    "with open(outside2_fname, 'rb') as f:\n",
    "    outside2 = pickle.load(f)\n",
    "    \n",
    "for k, v in outside1.items():\n",
    "    outside1[k] = v.astype(bool)\n",
    "    \n",
    "outside_dict = dict()\n",
    "for k in outside1.keys():\n",
    "    outside_dict[k] = (outside1[k] + outside2[k]).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1898 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1898/1898 [5:43:31<00:00, 10.86s/it]  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(dir_data, 'test.csv'))\n",
    "\n",
    "result = []\n",
    "model.eval()\n",
    "for idx in tqdm(range(len(df))):\n",
    "    img_path = os.path.join(dir_data, df.loc[idx, 'img_path'])\n",
    "    original_image = cv2.imread(img_path)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    outside = outside_dict[idx][np.newaxis, np.newaxis]\n",
    "    outside = torch.as_tensor(outside).float().to(device)\n",
    "    outside = torch.nn.functional.interpolate(\n",
    "                outside,\n",
    "                size=(540, 960),\n",
    "                mode='nearest'\n",
    "        )\n",
    "    outside = outside[0][0].cpu().numpy()\n",
    "    \n",
    "    # Stage 1\n",
    "    image = cv2.resize(original_image, (960, 540))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    preds, count_mat = slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "\n",
    "    # Stage 2\n",
    "    image = cv2.resize(original_image, (1200, 675))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    cur_preds, cur_count_mat = slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "    preds += F.interpolate(cur_preds, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "    count_mat += F.interpolate(cur_count_mat, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # Stage 3\n",
    "    image = cv2.resize(original_image, (1440, 810))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    cur_preds, cur_count_mat = slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "    preds += F.interpolate(cur_preds, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "    count_mat += F.interpolate(cur_count_mat, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    logits = preds / count_mat\n",
    "    _, predictions = logits.max(1)\n",
    "\n",
    "    predictions = predictions[0].cpu().numpy()\n",
    "    predictions[np.where(outside == 1)] = 12\n",
    "    predictions = predictions.astype(np.int32)\n",
    "    # class 0 ~ 11에 해당하는 경우에 마스크 형성 / 12(배경)는 제외하고 진행\n",
    "    for class_id in range(12):\n",
    "        class_mask = (predictions == class_id).astype(np.int32)\n",
    "        if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
    "            mask_rle = rle_encode(class_mask)\n",
    "            result.append(mask_rle)\n",
    "        else: # 마스크가 존재하지 않는 경우 -1\n",
    "            result.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>mask_rle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000_class_0</td>\n",
       "      <td>212629 3 212637 4 213588 22 214532 3 214545 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0000_class_1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0000_class_2</td>\n",
       "      <td>597 281 1557 281 2517 281 3476 282 4436 282 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0000_class_3</td>\n",
       "      <td>207753 8 208709 25 208777 9 209663 85 210618 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0000_class_4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22771</th>\n",
       "      <td>TEST_1897_class_7</td>\n",
       "      <td>152250 8 153208 14 153224 12 154166 35 155124 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22772</th>\n",
       "      <td>TEST_1897_class_8</td>\n",
       "      <td>95 539 678 125 1055 539 1639 124 2015 539 2599...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22773</th>\n",
       "      <td>TEST_1897_class_9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22774</th>\n",
       "      <td>TEST_1897_class_10</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22775</th>\n",
       "      <td>TEST_1897_class_11</td>\n",
       "      <td>192389 5 193340 21 194297 26 195255 29 196214 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22776 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                           mask_rle\n",
       "0       TEST_0000_class_0  212629 3 212637 4 213588 22 214532 3 214545 32...\n",
       "1       TEST_0000_class_1                                                 -1\n",
       "2       TEST_0000_class_2  597 281 1557 281 2517 281 3476 282 4436 282 53...\n",
       "3       TEST_0000_class_3  207753 8 208709 25 208777 9 209663 85 210618 9...\n",
       "4       TEST_0000_class_4                                                 -1\n",
       "...                   ...                                                ...\n",
       "22771   TEST_1897_class_7  152250 8 153208 14 153224 12 154166 35 155124 ...\n",
       "22772   TEST_1897_class_8  95 539 678 125 1055 539 1639 124 2015 539 2599...\n",
       "22773   TEST_1897_class_9                                                 -1\n",
       "22774  TEST_1897_class_10                                                 -1\n",
       "22775  TEST_1897_class_11  192389 5 193340 21 194297 26 195255 29 196214 ...\n",
       "\n",
       "[22776 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "submit['mask_rle'] = result\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./segformer-b5_multiscale_inference_pl2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
