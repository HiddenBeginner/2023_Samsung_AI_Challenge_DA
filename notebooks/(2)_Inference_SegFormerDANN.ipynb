{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**요약**\n",
    "- 학습된 SegFormerDANN로부터 테스트 데이터에 대한 예측을 합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Inputs:**\n",
    "- `dir_data`: 데이터가 있는 디렉토리\n",
    "- `dir_save`: 각 테스트 이미지에 대한 logit 파일을 저장할 폴더\n",
    "- `path_ckpt`: Inference에 사용할 SegFormerDANN 모델의 체크포인트 경로\n",
    "\n",
    "<br>\n",
    "\n",
    "**Outputs**:\n",
    "- f`{dir_save}/0000.pt`: 각 테스트 이미지에 대한 logit이 저장된 `pt` 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../data'\n",
    "dir_save = '../outputs/SegFormer_DANN'\n",
    "path_ckpt = '../ckpt/segformer_dann/best_ckpt_0048.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjin/miniconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-cityscapes-1024-1024 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([19, 768, 1, 1]) in the checkpoint and torch.Size([13, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([19]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-cityscapes-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([14, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from segformers.utils import rle_encode\n",
    "from segformers.networks import SegFormer\n",
    "from dann.DomainAdaptation import SegFormerDANN2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dict = torch.load(path_ckpt)\n",
    "model = SegFormerDANN2(segformer=SegFormer)\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "model.to(device);\n",
    "\n",
    "if not os.path.exists(dir_save):\n",
    "        os.makedirs(dir_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def slide_inference(images, model, num_classes=13, crop_size=(1024, 1024), stride=(768, 768)):\n",
    "    h_stride, w_stride = stride\n",
    "    h_crop, w_crop = crop_size\n",
    "    batch_size, _, h_img, w_img = images.size()\n",
    "\n",
    "    h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1\n",
    "    w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1\n",
    "    preds = images.new_zeros((batch_size, num_classes, h_img, w_img))\n",
    "    count_mat = images.new_zeros((batch_size, 1, h_img, w_img))\n",
    "    for h_idx in range(h_grids):\n",
    "        for w_idx in range(w_grids):\n",
    "            y1 = h_idx * h_stride\n",
    "            x1 = w_idx * w_stride\n",
    "            y2 = min(y1 + h_crop, h_img)\n",
    "            x2 = min(x1 + w_crop, w_img)\n",
    "            y1 = max(y2 - h_crop, 0)\n",
    "            x1 = max(x2 - w_crop, 0)\n",
    "            crop_img = images[:, :, y1:y2, x1:x2]\n",
    "\n",
    "            try:\n",
    "                crop_seg_logit = model(crop_img)[0]  # Try calling the model directly with crop_img\n",
    "            except TypeError:  # Catch the TypeError if model cannot be called with crop_img directly\n",
    "                crop_seg_logit = model(pixel_values=crop_img)[0]  # Call the model with pixel_values argument\n",
    "\n",
    "            crop_seg_logit = F.interpolate(\n",
    "                crop_seg_logit,\n",
    "                size=crop_size,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )\n",
    "            preds += F.pad(crop_seg_logit,\n",
    "                            (int(x1), int(preds.shape[3] - x2), int(y1),\n",
    "                            int(preds.shape[2] - y2)))\n",
    "\n",
    "            count_mat[:, :, y1:y2, x1:x2] += 1\n",
    "    assert (count_mat == 0).sum() == 0\n",
    "    seg_logits = preds / count_mat\n",
    "\n",
    "    return preds, count_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dir_data, 'test.csv'))\n",
    "\n",
    "result = []\n",
    "model.eval()\n",
    "for idx in tqdm(range(len(df))):\n",
    "    img_path = os.path.join(dir_data, df.loc[idx, 'img_path'])\n",
    "    original_image = cv2.imread(img_path)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "   \n",
    "    # Stage 1\n",
    "    image = cv2.resize(original_image, (960, 540))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    preds, count_mat = slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "\n",
    "    # Stage 2\n",
    "    image = cv2.resize(original_image, (1200, 675))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    cur_preds, cur_count_mat = slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "    preds += F.interpolate(cur_preds, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "    count_mat += F.interpolate(cur_count_mat, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # Stage 3\n",
    "    image = cv2.resize(original_image, (1440, 810))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    cur_preds, cur_count_mat = slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "    preds += F.interpolate(cur_preds, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "    count_mat += F.interpolate(cur_count_mat, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    logits = preds / count_mat\n",
    "    _, predictions = logits.max(1)\n",
    "\n",
    "    # logits (background 처리 전) 텐서를 .pt 파일로 저장\n",
    "    tensor_save_path = os.path.join(dir_save, f\"prediction_{idx}.pt\")\n",
    "    torch.save(logits, tensor_save_path)\n",
    "\n",
    "    predictions = predictions[0].cpu().numpy()\n",
    "    predictions = predictions.astype(np.int32)\n",
    "   \n",
    "    # class 0 ~ 11에 해당하는 경우에 마스크 형성 / 12(배경)는 제외하고 진행\n",
    "    for class_id in range(12):\n",
    "        class_mask = (predictions == class_id).astype(np.int32)\n",
    "        if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
    "            mask_rle = rle_encode(class_mask)\n",
    "            result.append(mask_rle)\n",
    "        else: # 마스크가 존재하지 않는 경우 -1\n",
    "            result.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "submit['mask_rle'] = result\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./segformer-dannv2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
