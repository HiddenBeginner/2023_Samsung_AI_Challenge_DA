{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**요약**\n",
    "- 학습된 SegFormer로부터 테스트 데이터에 대한 예측을 합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Inputs:**\n",
    "- `dir_data`: 데이터가 있는 디렉토리\n",
    "- `dir_save`: 각 테스트 이미지에 대한 logit 파일을 저장할 폴더\n",
    "- `path_ckpt`: Inference에 사용할 SegFormer 모델의 체크포인트 경로\n",
    "\n",
    "<br>\n",
    "\n",
    "**Outputs**:\n",
    "- f`{dir_save}/0000.npy`: 각 테스트 이미지에 대한 logit이 저장된 `npy` 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../data'\n",
    "\n",
    "dir_save = '../outputs/SegFormer'\n",
    "path_ckpt = '../ckpt/1695950569_pl/last_ckpt.bin'\n",
    "\n",
    "# SegFormer + Dice loss 모델에 대한 설정\n",
    "# dir_save = '../outputs/SegFormer_Dice'\n",
    "# path_ckpt = '../ckpt/1695990507/last_ckpt.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjin/miniconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-cityscapes-1024-1024 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([19, 768, 1, 1]) in the checkpoint and torch.Size([13, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([19]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-cityscapes-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([14, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from segformers.utils import print_env\n",
    "from segformers.networks import SegFormer\n",
    "from segformers.inference import _slide_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== System Information ==========\n",
      "DATE : 2023-10-04\n",
      "Pyton Version : 3.8.17\n",
      "PyTorch Version : 1.13.0\n",
      "OS : Linux 5.4.0-155-generic\n",
      "CPU spec : x86_64\n",
      "RAM spec : 503.73 GB\n",
      "Device 0:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 1:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 2:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 3:\n",
      "Name: NVIDIA DGX Display\n",
      "Total Memory: 3911.875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 4:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(dir_save, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dict = torch.load(path_ckpt)\n",
    "model = SegFormer\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "model.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1723/1898 [5:08:29<31:19, 10.74s/it]  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Not enough free space to write 26956800 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/dongjin/projects/da/notebooks/(1-3)_MultiScale_Inference.ipynb 셀 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/%281-3%29_MultiScale_Inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m logits \u001b[39m=\u001b[39m preds \u001b[39m/\u001b[39m count_mat\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/%281-3%29_MultiScale_Inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m _, predictions \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/%281-3%29_MultiScale_Inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m np\u001b[39m.\u001b[39;49msave(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(dir_save, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39m.npy\u001b[39;49m\u001b[39m'\u001b[39;49m), logits\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy())\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/numpy/lib/npyio.py:522\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[39mwith\u001b[39;00m file_ctx \u001b[39mas\u001b[39;00m fid:\n\u001b[1;32m    521\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(arr)\n\u001b[0;32m--> 522\u001b[0m     \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_array(fid, arr, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[1;32m    523\u001b[0m                        pickle_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(fix_imports\u001b[39m=\u001b[39;49mfix_imports))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/numpy/lib/format.py:722\u001b[0m, in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     \u001b[39mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m--> 722\u001b[0m         array\u001b[39m.\u001b[39;49mtofile(fp)\n\u001b[1;32m    723\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m         \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m numpy\u001b[39m.\u001b[39mnditer(\n\u001b[1;32m    725\u001b[0m                 array, flags\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mexternal_loop\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbuffered\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mzerosize_ok\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    726\u001b[0m                 buffersize\u001b[39m=\u001b[39mbuffersize, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;31mOSError\u001b[0m: Not enough free space to write 26956800 bytes"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(dir_data, 'test.csv'))\n",
    "\n",
    "result = []\n",
    "model.eval()\n",
    "for idx in tqdm(range(len(df))):\n",
    "    img_path = os.path.join(dir_data, df.loc[idx, 'img_path'])\n",
    "    original_image = cv2.imread(img_path)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Stage 1\n",
    "    image = cv2.resize(original_image, (960, 540))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    preds, count_mat = _slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "\n",
    "    # Stage 2\n",
    "    image = cv2.resize(original_image, (1200, 675))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    cur_preds, cur_count_mat = _slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "    preds += F.interpolate(cur_preds, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "    count_mat += F.interpolate(cur_count_mat, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # Stage 3\n",
    "    image = cv2.resize(original_image, (1440, 810))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    cur_preds, cur_count_mat = _slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "    preds += F.interpolate(cur_preds, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "    count_mat += F.interpolate(cur_count_mat, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    logits = preds / count_mat\n",
    "    _, predictions = logits.max(1)\n",
    "    \n",
    "    np.save(os.path.join(dir_save, f'{idx}.npy'), logits.cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
