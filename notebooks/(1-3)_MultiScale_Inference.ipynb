{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**요약**\n",
    "- 학습된 SegFormer로부터 테스트 데이터에 대한 예측을 합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Inputs:**\n",
    "- `dir_data`: 데이터가 있는 디렉토리\n",
    "- `dir_save`: 각 테스트 이미지에 대한 logit 파일을 저장할 폴더\n",
    "- `path_ckpt`: Inference에 사용할 SegFormer 모델의 체크포인트 경로\n",
    "\n",
    "<br>\n",
    "\n",
    "**Outputs**:\n",
    "- f`{dir_save}/0000.npy`: 각 테스트 이미지에 대한 logit이 저장된 `npy` 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../data'\n",
    "\n",
    "dir_save = '../outputs/SegFormer'\n",
    "path_ckpt = '../ckpt/1695950569_pl/last_ckpt.bin'\n",
    "\n",
    "# SegFormer + Dice loss 모델에 대한 설정\n",
    "# dir_save = '../outputs/SegFormer_Dice'\n",
    "# path_ckpt = '../ckpt/1695990507/last_ckpt.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjin/miniconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-cityscapes-1024-1024 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([19, 768, 1, 1]) in the checkpoint and torch.Size([13, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([19]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-cityscapes-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([14, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from segformers.utils import print_env\n",
    "from segformers.networks import SegFormer\n",
    "from segformers.inference import _slide_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== System Information ==========\n",
      "DATE : 2023-10-05\n",
      "Pyton Version : 3.8.17\n",
      "PyTorch Version : 1.13.0\n",
      "OS : Linux 5.4.0-155-generic\n",
      "CPU spec : x86_64\n",
      "RAM spec : 503.73 GB\n",
      "Device 0:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 1:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 2:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 3:\n",
      "Name: NVIDIA DGX Display\n",
      "Total Memory: 3911.875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 4:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "os.makedirs(dir_save, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dict = torch.load(path_ckpt)\n",
    "model = SegFormer\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "model.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dir_data, 'test.csv'))\n",
    "\n",
    "result = []\n",
    "model.eval()\n",
    "for idx in tqdm(range(len(df))):\n",
    "    img_path = os.path.join(dir_data, df.loc[idx, 'img_path'])\n",
    "    original_image = cv2.imread(img_path)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Stage 1\n",
    "    image = cv2.resize(original_image, (960, 540))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    preds, count_mat = _slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "\n",
    "    # Stage 2\n",
    "    image = cv2.resize(original_image, (1200, 675))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    cur_preds, cur_count_mat = _slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "    preds += F.interpolate(cur_preds, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "    count_mat += F.interpolate(cur_count_mat, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # Stage 3\n",
    "    image = cv2.resize(original_image, (1440, 810))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    cur_preds, cur_count_mat = _slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "    preds += F.interpolate(cur_preds, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "    count_mat += F.interpolate(cur_count_mat, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    logits = preds / count_mat\n",
    "    _, predictions = logits.max(1)\n",
    "    \n",
    "    np.save(os.path.join(dir_save, f'{idx}.npy'), logits[0].cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
