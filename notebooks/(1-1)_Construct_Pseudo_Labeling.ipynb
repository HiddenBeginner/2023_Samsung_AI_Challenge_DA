{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**요약**\n",
    "- 미세조정된 SegFormer와 배경 모델링으로부터 `train_target_image`에 대한 수도 레이블을 생성합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Inputs:**\n",
    "- `dir_data`: 데이터가 있는 디렉토리\n",
    "- `dir_save`: `train_target_image`에 대한 수도 레이블링을 저장할 디렉토리\n",
    "- `path_ckpt`: 수도 레이블링을 생성한 미세조정된 SegFormer 모델의 체크포인트\n",
    "- `outside_fname`: `train_target_image`의 각 사진에 대한 배경을 나타내는 이진 마스크를 담은 딕셔너리가 저장된 피클 파일 경로\n",
    "\n",
    "<br>\n",
    "\n",
    "**Outputs**:\n",
    "- f`{dir_save}/TRAIN_TARGET_0000.png`: `train_target_image`에 대한 수도 레이블\n",
    "- f`{dir_data}/full_pl.csv`: `train_source`, `train_target`, `val_source`의 메타 데이터를 담은 csv 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../data'\n",
    "dir_save = '../data/train_target_pl'\n",
    "path_ckpt = '../ckpt/1695288341/last_ckpt.bin'\n",
    "\n",
    "outside_fname = '../outputs/background_basesum.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjin/miniconda3/envs/pssc/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-cityscapes-1024-1024 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([19, 768, 1, 1]) in the checkpoint and torch.Size([13, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([19]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-cityscapes-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([14, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from segformers.utils import print_env\n",
    "from segformers.networks import SegFormer\n",
    "from segformers.inference import _slide_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== System Information ==========\n",
      "DATE : 2023-10-06\n",
      "Pyton Version : 3.8.17\n",
      "PyTorch Version : 1.13.0+cu117\n",
      "OS : Linux 5.4.0-155-generic\n",
      "CPU spec : x86_64\n",
      "RAM spec : 503.73 GB\n",
      "Device 0:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 1:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 2:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 3:\n",
      "Name: NVIDIA DGX Display\n",
      "Total Memory: 3911.875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 4:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dict = torch.load(path_ckpt)\n",
    "model = SegFormer\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "model.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(dir_save, exist_ok=True)\n",
    "\n",
    "with open(outside_fname, 'rb') as f:\n",
    "    outside_dict = pickle.load(f)\n",
    "    \n",
    "for k, v in outside_dict.items():\n",
    "    outside_dict[k] = v.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2923/2923 [8:46:01<00:00, 10.80s/it]  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(dir_data, 'train_target.csv'))\n",
    "\n",
    "model.eval()\n",
    "for idx in tqdm(range(len(df))):\n",
    "    img_path = os.path.join(dir_data, df.loc[idx, 'img_path'])\n",
    "    original_image = cv2.imread(img_path)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Stage 1\n",
    "    image = cv2.resize(original_image, (960, 540))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    preds, count_mat = _slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "\n",
    "    # Stage 2\n",
    "    image = cv2.resize(original_image, (1200, 675))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    cur_preds, cur_count_mat = _slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "    preds += F.interpolate(cur_preds, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "    count_mat += F.interpolate(cur_count_mat, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # Stage 3 \n",
    "    image = cv2.resize(original_image, (1440, 810))\n",
    "    image = A.Normalize()(image=image)['image']\n",
    "    images = torch.as_tensor(image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    cur_preds, cur_count_mat = _slide_inference(images, model, num_classes=13, stride=(50, 50), crop_size=(512, 512))\n",
    "    preds += F.interpolate(cur_preds, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "    count_mat += F.interpolate(cur_count_mat, size=(540, 960), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    logits = preds / count_mat\n",
    "    logits = F.interpolate(logits, size=(1080, 1920), mode=\"bilinear\", align_corners=False)[0]\n",
    "    entropies = torch.mean(-torch.log_softmax(logits, dim=0), dim=0).cpu()\n",
    "    mask = torch.argmax(logits, dim=0).cpu().numpy()\n",
    "\n",
    "    outside = outside_dict[idx]\n",
    "    mask[np.where(outside == 1)] = 12\n",
    "\n",
    "    mask[mask == 12] = 255\n",
    "    cv2.imwrite(os.path.join(dir_save, f\"TRAIN_TARGET_{str(idx).zfill(4)}.png\"), mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_df = pd.read_csv(os.path.join(dir_data, 'train_target.csv'))\n",
    "train_target_df['gt_path'] = train_target_df['id'].apply(lambda x: f'./train_target_pl/{x}.png')\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(dir_data, 'train_source.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(dir_data, 'val_source.csv'))\n",
    "data = pd.concat([train_df, valid_df, train_target_df], axis=0, ignore_index=True)\n",
    "\n",
    "data.to_csv(os.path.join(dir_data, 'full_pl.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
