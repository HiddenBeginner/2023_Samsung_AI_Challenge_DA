{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**요약**\n",
    "- Mask2Former를 미세조정합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Inputs:**\n",
    "- `dir_data`: 데이터가 있는 디렉토리\n",
    "- `dir_ckpt`: 학습된 모델을 저장할 디렉토리\n",
    "\n",
    "<br>\n",
    "\n",
    "**Outputs**:\n",
    "- f`{dir_ckpt}/1696079822`: 미세조정된 Mask2Former 모델 체크포인트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../data'\n",
    "dir_ckpt = '../ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjin/miniconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-cityscapes-1024-1024 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([19, 768, 1, 1]) in the checkpoint and torch.Size([13, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([19]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-cityscapes-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([14, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Mask2FormerImageProcessor\n",
    "\n",
    "from segformers.utils import seed_all, print_env\n",
    "from segformers.networks import Mask2Former\n",
    "from segformers.transforms import augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== System Information ==========\n",
      "DATE : 2023-10-04\n",
      "Pyton Version : 3.8.17\n",
      "PyTorch Version : 1.13.0\n",
      "OS : Linux 5.4.0-155-generic\n",
      "CPU spec : x86_64\n",
      "RAM spec : 503.73 GB\n",
      "Device 0:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 1:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 2:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 3:\n",
      "Name: NVIDIA DGX Display\n",
      "Total Memory: 3911.875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 4:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = Mask2FormerImageProcessor.from_pretrained(\"facebook/mask2former-swin-large-cityscapes-semantic\")\n",
    "image_processor.do_resize = False\n",
    "model = Mask2Former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceDataset(Dataset):\n",
    "    def __init__(self, root, csv_file, transform=None):\n",
    "        self.root = root\n",
    "        self.data = pd.read_csv(os.path.join(self.root, csv_file))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, self.data.loc[idx, 'img_path'])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask_path = os.path.join(self.root, self.data.loc[idx, 'gt_path'])\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask[mask == 255] = 12  # Considering pixel value 12 as background\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            augmented_image = augmented['image']\n",
    "            augmented_mask = augmented['mask']\n",
    "\n",
    "        return augmented_image, augmented_mask, augmented_image, augmented_mask,\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    images = inputs[0]\n",
    "    segmentation_maps = inputs[1]\n",
    "    # this function pads the inputs to the same size,\n",
    "    # and creates a pixel mask\n",
    "    # actually padding isn't required here since we are cropping\n",
    "    batch = image_processor(\n",
    "        images,\n",
    "        segmentation_maps=segmentation_maps,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    batch[\"original_images\"] = inputs[2]\n",
    "    batch[\"original_segmentation_maps\"] = inputs[3]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "train_dataset = SourceDataset(root=dir_data, csv_file='full.csv', transform=augmentation)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valid_dataset = SourceDataset(root=dir_data, csv_file='val_source.csv', transform=A.Compose([A.Resize(512, 512)]))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from segformers.utils import compute_mIoU\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        config,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.n_epochs = config['n_epochs']\n",
    "        self.dir_ckpt = config['dir_ckpt']\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, **config['optimizer'])\n",
    "\n",
    "        self.scheduler = CosineAnnealingWarmUpRestarts(self.optimizer, **config['scheduler'])\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.best_metric = 0.0\n",
    "        wandb.init(**config['wandb'], config=config)\n",
    "\n",
    "    def fit(self, train_loader, valid_loader):\n",
    "        for e in range(self.n_epochs):\n",
    "            train_scores = self.train(train_loader)\n",
    "            valid_scores = self.evaluate(valid_loader)\n",
    "\n",
    "            log = {'Epoch': e + 1, 'LR': self.scheduler.get_lr()[0]}\n",
    "            for k, v in train_scores.items():\n",
    "                log[f'train_{k}'] = v\n",
    "\n",
    "            for k, v in valid_scores.items():\n",
    "                log[f'valid_{k}'] = v\n",
    "\n",
    "            msg = ''\n",
    "            for k, v in log.items():\n",
    "                msg += f'{k}: {v:.4f} | '\n",
    "            print(msg[:-1])\n",
    "            wandb.log(log)\n",
    "\n",
    "            self.save(f'{self.dir_ckpt}/last_ckpt.bin')\n",
    "            if valid_scores['mIoU'] > self.best_metric:\n",
    "                self.best_metric = valid_scores['mIoU']\n",
    "                self.save(f'{self.dir_ckpt}/best_ckpt_{str(e+1).zfill(4)}.bin')\n",
    "                # Keep top 3 models\n",
    "                for path in sorted(glob(f'{self.dir_ckpt}/best_ckpt_*.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "\n",
    "            self.scheduler.step()\n",
    "        wandb.finish()\n",
    "\n",
    "    def train(self, loader):\n",
    "        self.model.train()\n",
    "        n = 0\n",
    "        scores = {'Loss': 0.0, 'mIoU': 0.0}\n",
    "        for it, inputs in enumerate(loader):\n",
    "            print(f\"{it} / {len(loader)}\", end='\\r')\n",
    "            outputs = self.model(\n",
    "                pixel_values=inputs[\"pixel_values\"].to(self.device),\n",
    "                mask_labels=[labels.to(self.device) for labels in inputs[\"mask_labels\"]],\n",
    "                class_labels=[labels.to(self.device) for labels in inputs[\"class_labels\"]],\n",
    "            )\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "             # get original images\n",
    "            original_images = inputs[\"original_images\"]\n",
    "            target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "            # predict segmentation maps\n",
    "            predicted = image_processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n",
    "            batch_size = len(original_images)\n",
    "            n += batch_size\n",
    "            scores['Loss'] += batch_size * loss.item()\n",
    "            for pred, gt in zip(predicted, inputs['original_segmentation_maps']):\n",
    "                scores['mIoU'] += compute_mIoU(pred, torch.as_tensor(gt, device=self.device))\n",
    "\n",
    "        for k, v in scores.items():\n",
    "            scores[k] = v / n\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        n = 0\n",
    "        scores = {'Loss': 0.0, 'mIoU': 0.0}\n",
    "        for inputs in loader:\n",
    "            outputs = self.model(\n",
    "                pixel_values=inputs[\"pixel_values\"].to(self.device),\n",
    "                mask_labels=[labels.to(self.device) for labels in inputs[\"mask_labels\"]],\n",
    "                class_labels=[labels.to(self.device) for labels in inputs[\"class_labels\"]],\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss          \n",
    "             # get original images\n",
    "            original_images = inputs[\"original_images\"]\n",
    "            target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "            # predict segmentation maps\n",
    "            predicted = image_processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n",
    "            batch_size = len(original_images)\n",
    "            n += batch_size\n",
    "            scores['Loss'] += batch_size * loss.item()\n",
    "            for pred, gt in zip(predicted, inputs['original_segmentation_maps']):\n",
    "                scores['mIoU'] += compute_mIoU(pred, torch.as_tensor(gt, device=self.device))\n",
    "\n",
    "        for k, v in scores.items():\n",
    "            scores[k] = v / n\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    \"\"\"\n",
    "    https://gaussian37.github.io/dl-pytorch-lr_scheduler/\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr) * self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) *\n",
    "                    (1 + math.cos(math.pi * (self.T_cur - self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "\n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dir_data': './data',\n",
    "    'dir_ckpt': './ckpt',\n",
    "    'seed': 0,\n",
    "    'n_epochs': 50,\n",
    "    'optimizer': {\n",
    "        'lr': 0.0,\n",
    "    },\n",
    "\n",
    "    'scheduler': {\n",
    "        'T_0': 50,\n",
    "        'T_mult': 1,\n",
    "        'eta_max': 0.00005,\n",
    "        'T_up': 5,\n",
    "        'gamma': 0.5,\n",
    "    },\n",
    "\n",
    "    'wandb': {\n",
    "        'project': 'DA',\n",
    "        'name': 'Mask2Former'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(config['seed'])\n",
    "run_id = int(datetime.timestamp(datetime.now()))\n",
    "config['run_id'] = run_id\n",
    "config['dir_ckpt'] = os.path.join(dir_ckpt, str(run_id))\n",
    "os.makedirs(config['dir_ckpt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdongjinlee\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dongjin/projects/da/notebooks/wandb/run-20230930_221704-cgqb657e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dongjinlee/DA/runs/cgqb657e' target=\"_blank\">Mask2Former</a></strong> to <a href='https://wandb.ai/dongjinlee/DA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dongjinlee/DA' target=\"_blank\">https://wandb.ai/dongjinlee/DA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dongjinlee/DA/runs/cgqb657e' target=\"_blank\">https://wandb.ai/dongjinlee/DA/runs/cgqb657e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(model, config)\n",
    "trainer.load('/home/dongjin/projects/da/ckpt/1695982337/last_ckpt.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.0000 | LR: 0.0000 | train_Loss: 16.7380 | train_mIoU: 0.6635 | valid_Loss: 18.8804 | valid_mIoU: 0.6510 |\n",
      "Epoch: 2.0000 | LR: 0.0000 | train_Loss: 15.8349 | train_mIoU: 0.6786 | valid_Loss: 18.3618 | valid_mIoU: 0.6506 |\n",
      "Epoch: 3.0000 | LR: 0.0000 | train_Loss: 15.4554 | train_mIoU: 0.6837 | valid_Loss: 18.2946 | valid_mIoU: 0.6503 |\n",
      "Epoch: 4.0000 | LR: 0.0000 | train_Loss: 15.2329 | train_mIoU: 0.6853 | valid_Loss: 19.0883 | valid_mIoU: 0.6540 |\n",
      "Epoch: 5.0000 | LR: 0.0000 | train_Loss: 15.0496 | train_mIoU: 0.6906 | valid_Loss: 17.9825 | valid_mIoU: 0.6685 |\n",
      "Epoch: 6.0000 | LR: 0.0001 | train_Loss: 14.8931 | train_mIoU: 0.6922 | valid_Loss: 18.3842 | valid_mIoU: 0.6464 |\n",
      "Epoch: 7.0000 | LR: 0.0000 | train_Loss: 14.5293 | train_mIoU: 0.6999 | valid_Loss: 17.9689 | valid_mIoU: 0.6535 |\n",
      "Epoch: 8.0000 | LR: 0.0000 | train_Loss: 14.0966 | train_mIoU: 0.7116 | valid_Loss: 18.2261 | valid_mIoU: 0.6613 |\n",
      "Epoch: 9.0000 | LR: 0.0000 | train_Loss: 13.9868 | train_mIoU: 0.7139 | valid_Loss: 17.8517 | valid_mIoU: 0.6605 |\n",
      "Epoch: 10.0000 | LR: 0.0000 | train_Loss: 13.7590 | train_mIoU: 0.7166 | valid_Loss: 18.3203 | valid_mIoU: 0.6488 |\n",
      "Epoch: 11.0000 | LR: 0.0000 | train_Loss: 14.9279 | train_mIoU: 0.6907 | valid_Loss: 17.6149 | valid_mIoU: 0.6603 |\n",
      "Epoch: 12.0000 | LR: 0.0000 | train_Loss: 14.5791 | train_mIoU: 0.6980 | valid_Loss: 18.1536 | valid_mIoU: 0.6650 |\n",
      "Epoch: 13.0000 | LR: 0.0000 | train_Loss: 14.3321 | train_mIoU: 0.6984 | valid_Loss: 17.2516 | valid_mIoU: 0.6773 |\n",
      "Epoch: 14.0000 | LR: 0.0000 | train_Loss: 13.9751 | train_mIoU: 0.7086 | valid_Loss: 17.2372 | valid_mIoU: 0.6763 |\n",
      "Epoch: 15.0000 | LR: 0.0000 | train_Loss: 13.9576 | train_mIoU: 0.7098 | valid_Loss: 17.3036 | valid_mIoU: 0.6624 |\n",
      "Epoch: 16.0000 | LR: 0.0000 | train_Loss: 13.6887 | train_mIoU: 0.7153 | valid_Loss: 17.5003 | valid_mIoU: 0.6587 |\n",
      "Epoch: 17.0000 | LR: 0.0000 | train_Loss: 13.6667 | train_mIoU: 0.7130 | valid_Loss: 16.8031 | valid_mIoU: 0.6757 |\n",
      "Epoch: 18.0000 | LR: 0.0000 | train_Loss: 13.4176 | train_mIoU: 0.7200 | valid_Loss: 17.1441 | valid_mIoU: 0.6728 |\n",
      "Epoch: 19.0000 | LR: 0.0000 | train_Loss: 13.1992 | train_mIoU: 0.7253 | valid_Loss: 16.9204 | valid_mIoU: 0.6746 |\n",
      "Epoch: 20.0000 | LR: 0.0000 | train_Loss: 13.1504 | train_mIoU: 0.7238 | valid_Loss: 16.5874 | valid_mIoU: 0.6874 |\n",
      "Epoch: 21.0000 | LR: 0.0000 | train_Loss: 13.0721 | train_mIoU: 0.7262 | valid_Loss: 16.3740 | valid_mIoU: 0.6870 |\n",
      "Epoch: 22.0000 | LR: 0.0000 | train_Loss: 12.6530 | train_mIoU: 0.7371 | valid_Loss: 16.2743 | valid_mIoU: 0.6929 |\n",
      "Epoch: 23.0000 | LR: 0.0000 | train_Loss: 12.7717 | train_mIoU: 0.7334 | valid_Loss: 16.2308 | valid_mIoU: 0.6785 |\n",
      "Epoch: 24.0000 | LR: 0.0000 | train_Loss: 12.5753 | train_mIoU: 0.7373 | valid_Loss: 16.1220 | valid_mIoU: 0.6823 |\n",
      "Epoch: 25.0000 | LR: 0.0000 | train_Loss: 12.4163 | train_mIoU: 0.7395 | valid_Loss: 16.2879 | valid_mIoU: 0.7018 |\n",
      "Epoch: 26.0000 | LR: 0.0000 | train_Loss: 12.2043 | train_mIoU: 0.7455 | valid_Loss: 16.0917 | valid_mIoU: 0.7057 |\n",
      "Epoch: 27.0000 | LR: 0.0000 | train_Loss: 12.0388 | train_mIoU: 0.7507 | valid_Loss: 15.4820 | valid_mIoU: 0.7050 |\n",
      "Epoch: 28.0000 | LR: 0.0000 | train_Loss: 12.1700 | train_mIoU: 0.7442 | valid_Loss: 16.1035 | valid_mIoU: 0.7075 |\n",
      "Epoch: 29.0000 | LR: 0.0000 | train_Loss: 11.9240 | train_mIoU: 0.7501 | valid_Loss: 15.7357 | valid_mIoU: 0.7087 |\n",
      "Epoch: 30.0000 | LR: 0.0000 | train_Loss: 11.8478 | train_mIoU: 0.7539 | valid_Loss: 15.4605 | valid_mIoU: 0.7054 |\n",
      "14 / 333\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb 셀 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_loader, valid_loader)\n",
      "\u001b[1;32m/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb 셀 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, train_loader, valid_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m         train_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(train_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m         valid_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(valid_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m         log \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m'\u001b[39m: e \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLR\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mget_lr()[\u001b[39m0\u001b[39m]}\n",
      "\u001b[1;32m/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb 셀 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m  \u001b[39m# get original images\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.223.36.200/home/dongjin/projects/da/notebooks/2023-09-28_Mask2Former.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m original_images \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39moriginal_images\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     adamw(params_with_grad,\n\u001b[1;32m    163\u001b[0m           grads,\n\u001b[1;32m    164\u001b[0m           exp_avgs,\n\u001b[1;32m    165\u001b[0m           exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m           max_exp_avg_sqs,\n\u001b[1;32m    167\u001b[0m           state_steps,\n\u001b[1;32m    168\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    169\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    170\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    171\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m func(params,\n\u001b[1;32m    220\u001b[0m      grads,\n\u001b[1;32m    221\u001b[0m      exp_avgs,\n\u001b[1;32m    222\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    223\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    224\u001b[0m      state_steps,\n\u001b[1;32m    225\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    226\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    227\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    228\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    229\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    230\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    232\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/optim/adamw.py:267\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    264\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    266\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[39m# Perform stepweight decay\u001b[39;00m\n\u001b[1;32m    270\u001b[0m param\u001b[39m.\u001b[39mmul_(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m lr \u001b[39m*\u001b[39m weight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.fit(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
