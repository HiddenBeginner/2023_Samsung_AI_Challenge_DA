{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**요약**\n",
    "- Mask2Former를 미세조정합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Inputs:**\n",
    "- `dir_data`: 데이터가 있는 디렉토리\n",
    "- `dir_ckpt`: 학습된 모델을 저장할 디렉토리\n",
    "\n",
    "<br>\n",
    "\n",
    "**Outputs**:\n",
    "- f`{dir_ckpt}/1696079822`: 미세조정된 Mask2Former 모델 체크포인트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../data'\n",
    "dir_ckpt = '../ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjin/miniconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-cityscapes-1024-1024 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([19, 768, 1, 1]) in the checkpoint and torch.Size([13, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([19]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-cityscapes-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([14, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Mask2FormerImageProcessor\n",
    "\n",
    "from segformers.utils import seed_all, print_env\n",
    "from segformers.networks import Mask2Former\n",
    "from segformers.transforms import augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== System Information ==========\n",
      "DATE : 2023-10-05\n",
      "Pyton Version : 3.8.17\n",
      "PyTorch Version : 1.13.0\n",
      "OS : Linux 5.4.0-155-generic\n",
      "CPU spec : x86_64\n",
      "RAM spec : 503.73 GB\n",
      "Device 0:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 1:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 2:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 3:\n",
      "Name: NVIDIA DGX Display\n",
      "Total Memory: 3911.875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n",
      "Device 4:\n",
      "Name: NVIDIA A100-SXM4-40GB\n",
      "Total Memory: 40536.1875 MB\n",
      "Driver Version: 470.199.02\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = Mask2FormerImageProcessor.from_pretrained(\"facebook/mask2former-swin-large-cityscapes-semantic\")\n",
    "image_processor.do_resize = False\n",
    "model = Mask2Former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceDataset(Dataset):\n",
    "    def __init__(self, root, csv_file, transform=None):\n",
    "        self.root = root\n",
    "        self.data = pd.read_csv(os.path.join(self.root, csv_file))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, self.data.loc[idx, 'img_path'])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask_path = os.path.join(self.root, self.data.loc[idx, 'gt_path'])\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask[mask == 255] = 12  # Considering pixel value 12 as background\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            augmented_image = augmented['image']\n",
    "            augmented_mask = augmented['mask']\n",
    "\n",
    "        return augmented_image, augmented_mask, augmented_image, augmented_mask,\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    images = inputs[0]\n",
    "    segmentation_maps = inputs[1]\n",
    "    # this function pads the inputs to the same size,\n",
    "    # and creates a pixel mask\n",
    "    # actually padding isn't required here since we are cropping\n",
    "    batch = image_processor(\n",
    "        images,\n",
    "        segmentation_maps=segmentation_maps,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    batch[\"original_images\"] = inputs[2]\n",
    "    batch[\"original_segmentation_maps\"] = inputs[3]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "train_dataset = SourceDataset(root=dir_data, csv_file='full.csv', transform=augmentation)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valid_dataset = SourceDataset(root=dir_data, csv_file='val_source.csv', transform=A.Compose([A.Resize(512, 512)]))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from segformers.utils import compute_mIoU\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        config,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.n_epochs = config['n_epochs']\n",
    "        self.dir_ckpt = config['dir_ckpt']\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, **config['optimizer'])\n",
    "\n",
    "        self.scheduler = CosineAnnealingWarmUpRestarts(self.optimizer, **config['scheduler'])\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.best_metric = 0.0\n",
    "        wandb.init(**config['wandb'], config=config)\n",
    "\n",
    "    def fit(self, train_loader, valid_loader):\n",
    "        for e in range(self.n_epochs):\n",
    "            train_scores = self.train(train_loader)\n",
    "            valid_scores = self.evaluate(valid_loader)\n",
    "\n",
    "            log = {'Epoch': e + 1, 'LR': self.scheduler.get_lr()[0]}\n",
    "            for k, v in train_scores.items():\n",
    "                log[f'train_{k}'] = v\n",
    "\n",
    "            for k, v in valid_scores.items():\n",
    "                log[f'valid_{k}'] = v\n",
    "\n",
    "            msg = ''\n",
    "            for k, v in log.items():\n",
    "                msg += f'{k}: {v:.4f} | '\n",
    "            print(msg[:-1])\n",
    "            wandb.log(log)\n",
    "\n",
    "            self.save(f'{self.dir_ckpt}/last_ckpt.bin')\n",
    "            if valid_scores['mIoU'] > self.best_metric:\n",
    "                self.best_metric = valid_scores['mIoU']\n",
    "                self.save(f'{self.dir_ckpt}/best_ckpt_{str(e+1).zfill(4)}.bin')\n",
    "                # Keep top 3 models\n",
    "                for path in sorted(glob(f'{self.dir_ckpt}/best_ckpt_*.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "\n",
    "            self.scheduler.step()\n",
    "        wandb.finish()\n",
    "\n",
    "    def train(self, loader):\n",
    "        self.model.train()\n",
    "        n = 0\n",
    "        scores = {'Loss': 0.0, 'mIoU': 0.0}\n",
    "        for it, inputs in enumerate(loader):\n",
    "            print(f\"{it} / {len(loader)}\", end='\\r')\n",
    "            outputs = self.model(\n",
    "                pixel_values=inputs[\"pixel_values\"].to(self.device),\n",
    "                mask_labels=[labels.to(self.device) for labels in inputs[\"mask_labels\"]],\n",
    "                class_labels=[labels.to(self.device) for labels in inputs[\"class_labels\"]],\n",
    "            )\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "             # get original images\n",
    "            original_images = inputs[\"original_images\"]\n",
    "            target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "            # predict segmentation maps\n",
    "            predicted = image_processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n",
    "            batch_size = len(original_images)\n",
    "            n += batch_size\n",
    "            scores['Loss'] += batch_size * loss.item()\n",
    "            for pred, gt in zip(predicted, inputs['original_segmentation_maps']):\n",
    "                scores['mIoU'] += compute_mIoU(pred, torch.as_tensor(gt, device=self.device))\n",
    "\n",
    "        for k, v in scores.items():\n",
    "            scores[k] = v / n\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        n = 0\n",
    "        scores = {'Loss': 0.0, 'mIoU': 0.0}\n",
    "        for inputs in loader:\n",
    "            outputs = self.model(\n",
    "                pixel_values=inputs[\"pixel_values\"].to(self.device),\n",
    "                mask_labels=[labels.to(self.device) for labels in inputs[\"mask_labels\"]],\n",
    "                class_labels=[labels.to(self.device) for labels in inputs[\"class_labels\"]],\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss          \n",
    "             # get original images\n",
    "            original_images = inputs[\"original_images\"]\n",
    "            target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "            # predict segmentation maps\n",
    "            predicted = image_processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n",
    "            batch_size = len(original_images)\n",
    "            n += batch_size\n",
    "            scores['Loss'] += batch_size * loss.item()\n",
    "            for pred, gt in zip(predicted, inputs['original_segmentation_maps']):\n",
    "                scores['mIoU'] += compute_mIoU(pred, torch.as_tensor(gt, device=self.device))\n",
    "\n",
    "        for k, v in scores.items():\n",
    "            scores[k] = v / n\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    \"\"\"\n",
    "    https://gaussian37.github.io/dl-pytorch-lr_scheduler/\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr) * self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) *\n",
    "                    (1 + math.cos(math.pi * (self.T_cur - self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "\n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 0,\n",
    "    'n_epochs': 50,\n",
    "    'optimizer': {\n",
    "        'lr': 0.0,\n",
    "    },\n",
    "\n",
    "    'scheduler': {\n",
    "        'T_0': 50,\n",
    "        'T_mult': 1,\n",
    "        'eta_max': 0.00005,\n",
    "        'T_up': 5,\n",
    "        'gamma': 0.5,\n",
    "    },\n",
    "\n",
    "    'wandb': {\n",
    "        'project': 'DA',\n",
    "        'name': 'Mask2Former'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(config['seed'])\n",
    "run_id = int(datetime.timestamp(datetime.now()))\n",
    "config['run_id'] = run_id\n",
    "config['dir_ckpt'] = os.path.join(dir_ckpt, str(run_id))\n",
    "os.makedirs(config['dir_ckpt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
