{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**요약**\n",
    "- 미세조정된 Mask2Former를 사용하여 테스트 이미지에 대해 추론을 합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Inputs:**\n",
    "- `dir_data`: 데이터가 있는 디렉토리\n",
    "- `dir_save`: 예측 파일이 저장되는 디렉토리\n",
    "- `dir_ckpt`: 학습된 모델을 저장할 디렉토리\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Outputs**:\n",
    "- f`{dir_save}/Mask2Former.csv`: 미세조정된 Mask2Former 모델 체크포인트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../data'\n",
    "dir_save = '../outputs/Mask2Former'\n",
    "path_ckpt = '../ckpt/1696079822/last_ckpt.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongjin/miniconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-cityscapes-1024-1024 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([19, 768, 1, 1]) in the checkpoint and torch.Size([13, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([19]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-cityscapes-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([14, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import Mask2FormerImageProcessor\n",
    "from segformers.utils import rle_encode\n",
    "from segformers.networks import Mask2Former\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dict = torch.load(path_ckpt)\n",
    "model = Mask2Former\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "model.to(device);\n",
    "\n",
    "image_processor = Mask2FormerImageProcessor.from_pretrained(\"facebook/mask2former-swin-large-cityscapes-semantic\")\n",
    "image_processor.do_resize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1898/1898 [56:41<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(dir_data, 'test.csv'))\n",
    "\n",
    "result = []\n",
    "for idx in tqdm(range(len(df))):\n",
    "    img_path = os.path.join(dir_data, df.loc[idx, 'img_path'])\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (1920, 1080))\n",
    "    normalized_image = A.Normalize()(image=image)['image']\n",
    "\n",
    "    images = torch.as_tensor(normalized_image, dtype=torch.float, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        class_queries_logits = outputs.class_queries_logits\n",
    "        masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "        masks_queries_logits = torch.nn.functional.interpolate(\n",
    "                    masks_queries_logits, size=(384, 384), mode=\"bilinear\", align_corners=False\n",
    "                )\n",
    "        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n",
    "        masks_probs = masks_queries_logits.sigmoid()\n",
    "        segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n",
    "        logits = crop_seg_logit = F.interpolate(\n",
    "                    segmentation,\n",
    "                    size=(1080, 1920),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False\n",
    "                )\n",
    "\n",
    "    masks = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "    masks = cv2.resize(masks, (960, 540), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    predictions = masks.astype(np.int32)\n",
    "    for class_id in range(12):\n",
    "        class_mask = (predictions == class_id).astype(np.int32)\n",
    "        if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
    "            mask_rle = rle_encode(class_mask)\n",
    "            result.append(mask_rle)\n",
    "        else: # 마스크가 존재하지 않는 경우 -1\n",
    "            result.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "submit['mask_rle'] = result\n",
    "submit.to_csv(os.path.join(dir_save, 'Mask2Former.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
